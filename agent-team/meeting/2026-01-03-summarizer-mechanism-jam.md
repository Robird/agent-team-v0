# Summarizer 机制畅谈会

> **日期**：2026-01-03
> **主持人**：Moderator
> **标签**：#design #decision
> **层级**：Resolve-Tier → Plan-Tier

---

## 背景

LLM Agent 上下文约 100K 时触发压缩，工作记忆捉襟见肘。人类靠 "View-And-Take" 模式——带目的阅读、当场分析、只存关键信息。LLM 的 `read_file` 工具则是全文灌入上下文，粗放管理。

## 提议要点

1. **Summarizer Subagent**：提供定制化摘要服务
2. **缓存机制**：基于文件哈希，记录 `{文件-哈希-问题-答案文件}` 四元组
3. **实现路径**：
   - 近期：Copilot-Chat CustomAgent + 提示词工程 + 专用目录
   - 远期：CLI 工具 + 二阶知识缓存（认知依赖追踪）
4. **检索方式**：索引分页 + 多 Agent 并行/串行查询

## 待解决问题

1. **Resolve-Tier**：值得做吗？投入产出比？
2. **Plan-Tier**：最高性价比的实现路径？

---

## 会议记录

### Seeker 发言（本质追问）

**问题本质**：不是"空间不够" vs "用得不好"的二元对立，而是**阅读与思考的时序耦合被打断**——人类边读边处理（Processing-While-Reading），LLM 先全部加载再处理（Load-Then-Process）。

**方案定位**：Summarizer 解决的是**中间层问题**——在工具设计假设与上下文爆满之间提供缓冲层。

**关键遗漏假设**：
- 摘要依赖消费者意图（为谁摘要？）
- 四元组缺失认知上下文维度
- 缓存命中率假设未验证

**替代思路**：智能 read_file、结构化知识库、增量式上下文管理、View-And-Take 原生化——与 Summarizer 可能互补。

**投入产出框架**：信息密度提升、任务成功率、延迟成本、缓存命中率、实现复杂度。

**核心判断**：**有条件地值得**——聚焦高频场景、设计意图传递机制、**先做手动版本验证概念**。

**最关键追问**：**Summarizer 如何知道"为谁摘要"？**

---

### Curator 发言（用户体验视角）

**Agent 调用体验的摩擦点**：
- **决策负担**：何时用 Summarizer vs 直接 read_file？
- **信任成本**：一旦信任崩塌，会退回 read_file

**适用场景**：
- ✅ 甜蜜点：导航型、确认型、扫描型阅读——有明确问题的定向阅读
- ❌ 不适合：创作型、调试型、学习型阅读——需要完整上下文

**人类监护人体验三维度**：透明度、可控性、成本感知

**核心建议**：**"宁可少摘要，不可错摘要"**——保守策略 + 明确标注不确定性 + 提供逃生通道 + 建立反馈闭环。


### Seeker 发言

#### 一、问题本质追问：上下文不够 vs 用得不好？

这是一个伪二元对立。让我解构：

**观察**："100K 触发压缩"是症状，不是病因。真正的问题是：

> **当前 read_file 模式破坏了"认知处理的时序性"**

人类阅读是 **Processing-While-Reading**——边读边思考边遗忘。LLM 的 read_file 是 **Load-Then-Process**——先全部加载，然后才开始处理。这导致：
- 加载时不知道要留什么（没有问题引导）
- 处理时信息已经稀释在巨大上下文中

**核心洞见**：问题不是"空间不够"也不是"用得不好"，而是 **"阅读与思考的时序耦合被打断"**。

#### 二、Summarizer 方案解决的是表象还是根因？

**直接判断**：解决的是**中间层问题**，既非表象亦非根因。

**根因层**（更深）：
- LLM 架构限制：无法在 token 流动过程中动态更新"什么值得保留"的判断
- 工具设计假设：read_file 假设"全部内容等价重要"

**表象层**（更浅）：
- 上下文爆满
- 压缩触发导致信息丢失

**Summarizer 位置**：在"工具设计假设"和"上下文爆满"之间，提供**缓冲层**。

**被遗漏的关键假设**：

1. **假设：摘要能保留关键信息**
   - 反例：代码阅读场景，"关键"取决于后续要做什么
   - Summarizer 必须知道**消费者意图**才能有效摘要

2. **假设：文件-哈希-问题-答案 四元组是 SSOT**
   - 问题：同一问题在不同项目上下文中可能需要不同答案
   - 缺失维度：**认知上下文**（"我正在做什么"）

3. **假设：缓存命中率足够高**
   - 这取决于**问题空间的稳定性**
   - 探索性任务 vs 重复性任务的缓存价值差异巨大

#### 三、替代思路分析

| 方案 | 核心思路 | 优势 | 劣势 | 与 Summarizer 关系 |
|:-----|:---------|:-----|:-----|:------------------|
| **Summarizer Subagent** | 前置处理，提供定制摘要 | 意图驱动、可缓存 | 增加延迟、需要调度 | 本提案 |
| **智能 read_file** | 工具内置摘要能力 | 零架构变更 | 缺乏缓存、每次重算 | 可作为 Summarizer 的同步版本 |
| **结构化知识库** | RAG 模式，向量检索 | 语义匹配好 | 索引维护成本、检索噪音 | 互补：适合跨文件问题 |
| **增量式上下文管理** | 类似 Git diff，只传变更 | 减少重复传输 | 需要状态追踪 | 互补：减少 Summarizer 调用 |
| **View-And-Take 原生化** | 模拟人类阅读模式 | 最接近人类认知 | 需要架构变革 | 长期愿景方向 |

**我的判断**：Summarizer 是**务实的中间层方案**，但它的设计需要回答一个核心问题：

> **Summarizer 如何知道"为谁摘要"？**

如果只是"给我这个文件的摘要"→ 退化为通用压缩，价值有限。
如果是"为了完成 X 任务，告诉我这个文件中与 X 相关的信息"→ 需要**意图传递机制**。

#### 四、投入产出比分析框架

**我提议的评估维度**：

| 维度 | 度量 | 阈值建议 |
|:-----|:-----|:---------|
| **信息密度提升** | (摘要后有用信息/摘要长度) vs (原文有用信息/原文长度) | 需 >2x 才有意义 |
| **任务成功率** | 使用 Summarizer 前后的任务完成质量 | 可测量的验收点 |
| **延迟成本** | Summarizer 调用增加的等待时间 | <3s 可接受（人类心理阈值） |
| **缓存命中率** | 命中次数/总调用次数 | >30% 才能摊薄成本 |
| **实现复杂度** | 新增代码量、维护负担 | 主观判断 |

**投入产出比公式草案**：

$$ROI = \frac{\Delta(\text{任务成功率}) \times \text{任务频率}}{\text{实现成本} + \text{维护成本} + \text{延迟成本}}$$

**关键问题**：我们能否设计一个**最小可验证实验**来检验核心假设？

#### 五、盲点与关键问题

1. **Summarizer 的 Summarizer 问题**
   - 如果文件很多，索引本身也会爆炸
   - 需要**层级摘要结构**（摘要的摘要）

2. **认知上下文传递**
   - "问题"字段太弱，需要传递"我正在做什么任务"
   - 这指向更深层的设计：**Agent 意图系统**

3. **缓存失效策略**
   - 文件哈希变了好判断
   - 但"认知上下文变了导致同样问题需要不同答案"怎么处理？

4. **与现有工具的关系**
   - `semantic_search` 已经提供了部分功能
   - Summarizer vs semantic_search 的分工边界在哪？

5. **冷启动问题**
   - 新项目/新领域，缓存为空，退化为实时处理
   - 这时的体验如何保证？

#### 六、我的判断

**值得做吗？** 

**有条件地值得**。条件是：

1. 聚焦于**高频重复阅读场景**（如：项目 AGENTS.md、核心设计文档）
2. 设计**意图传递机制**，避免退化为通用压缩
3. 从**最小实验**开始验证核心假设

**不建议做的方向**：
- 试图覆盖所有阅读场景
- 在没有验证缓存命中率假设前投入完整实现

**我的核心建议**：

> **先做一个手动版本验证概念**——人工为 3-5 个高频文档撰写"任务导向摘要"，观察实际使用效果，再决定是否自动化。

这符合 Artifact-Tiers 方法论：先在 Resolve-Tier 确认价值，再进入 Shape-Tier。

---

### Curator 发言

我从用户体验视角切入，分析 Summarizer 机制对两类用户（Agent 和人类监护人）的影响。

#### 一、Agent 作为"被服务者"的调用体验

**核心体验模型**：调用 Summarizer 本质上是 **"委托阅读"** ——我不亲自读，让别人帮我读。

**体验流程分析**：

```
触发意图 → 表达需求 → 等待结果 → 消化结果 → 决定下一步
```

**摩擦点定位**：

| 阶段 | 理想体验 | 潜在摩擦 |
|:-----|:---------|:---------|
| **触发意图** | 自然地意识到"我需要摘要" | 何时该调 Summarizer vs 直接 read_file？决策负担 |
| **表达需求** | 用自然语言描述关心什么 | "问题"如何表达？太宽泛无效，太具体难复用 |
| **等待结果** | 几乎即时 | 缓存未命中时的延迟感知 |
| **消化结果** | 摘要直接可用 | 摘要不够/过多时需要追问或补充阅读 |
| **决定下一步** | 清晰知道是否需要深入 | 不确定摘要是否遗漏关键信息 |

**关键体验洞见**：

> **信任成本**：Agent 必须信任 Summarizer 没有遗漏关键信息。这种信任的建立需要时间和正反馈循环。

如果 Agent 多次发现"摘要说没有 X，但实际上有 X"，信任崩塌后会退回直接 read_file。

#### 二、Agent 何时会/不会主动调用？

**会主动调用的场景**（高价值）：

| 场景 | 特征 | 示例 |
|:-----|:-----|:-----|
| **导航型阅读** | 想知道"这里有没有我要的东西" | 阅读 AGENTS.md 找特定操作指南入口 |
| **确认型阅读** | 记得有这东西，想确认细节 | 确认某个 API 的参数约束 |
| **扫描型阅读** | 大量文件快速过滤 | 在 10 个候选文件中找到相关的 2 个 |

**不会调用的场景**（低价值或不适合）：

| 场景 | 原因 |
|:-----|:-----|
| **创作型阅读** | 需要完整上下文才能做出创造性决策 |
| **调试型阅读** | Bug 可能藏在任何细节中，摘要会丢信息 |
| **学习型阅读** | 第一次接触领域，不知道该问什么问题 |
| **代码修改** | 修改需要精确的行级上下文 |

**关键启发**：

> Summarizer 的甜蜜点是 **"有明确问题的定向阅读"**，而非"开放探索"。

这意味着：**Agent 需要先有问题，才能用好 Summarizer**。对于"我不知道该问什么"的场景，Summarizer 帮不上忙。

#### 三、缓存命中 vs 未命中的体验差异

**体验对比矩阵**：

| 维度 | 缓存命中 | 缓存未命中 |
|:-----|:---------|:-----------|
| **延迟** | ~0.1s（文件读取） | ~5-15s（LLM 调用） |
| **成本** | 几乎为零 | 一次 LLM 请求 |
| **确定性** | 100% 可预测 | 结果可能有随机性 |
| **透明度** | 可审计（答案有记录） | 黑盒生成 |

**体验设计建议**：

1. **命中时**：静默返回，无需额外信息
2. **未命中时**：
   - 告知"正在生成新摘要"（设定预期）
   - 完成后告知"已缓存，下次更快"（建立信任）
   - 可选：显示缓存键，便于调试

**等待体验的心理学**：

> **3秒法则**：超过 3 秒的等待需要进度反馈，超过 10 秒需要解释原因。

如果 Summarizer 调用经常超过 10 秒，Agent（和监护人）会形成"这个工具很慢"的心智模型，降低使用意愿。

#### 四、Implementer 视角：我希望 Summarizer 提供什么？

假设我是一个需要频繁阅读代码的 Implementer，我的期望是：

**核心需求**：**省时间，不丢关键信息**

**具体服务期望**：

| 服务类型 | 描述 | 示例问题 |
|:---------|:-----|:---------|
| **结构鸟瞰** | 告诉我这个文件的骨架 | "这个类有哪些公开方法？" |
| **依赖地图** | 这个文件依赖什么、被谁依赖 | "改这个文件可能影响哪些地方？" |
| **关键约束** | 这里有什么不变量/前置条件 | "调用这个 API 有什么注意事项？" |
| **历史线索** | 这块代码为什么这样写 | "这个奇怪的 workaround 是为了解决什么？" |
| **定位服务** | 我要改 X 功能，应该看哪里 | "实现用户认证的代码在哪？" |

**不期望的服务**：

- ❌ 逐行解释代码（我自己能读）
- ❌ 重复文档中已有的信息（浪费 token）
- ❌ 不确定的猜测（宁可说"不知道"）

**理想交互模式**：

```
我: @Summarizer 阅读 src/StateJournal/Workspace.cs，告诉我：
    1. 核心职责是什么？
    2. 有哪些公开 API？
    3. 使用时有什么约束？

Summarizer: 
    [结构化的简洁回答]
    [如果有不确定的地方，明确标注]
    [建议：如果你要做 X，可能还需要看 Y 文件]
```

#### 五、人类监护人的体验影响

**三个关键维度**：

##### 5.1 透明度（我知道发生了什么吗？）

| 信息 | 重要性 | 呈现方式建议 |
|:-----|:-------|:-------------|
| 是否调用了 Summarizer | 中 | 日志/标记 |
| 缓存命中还是新生成 | 高 | 明确标注 |
| Summarizer 的回答内容 | 高 | 可展开查看 |
| Summarizer 消耗的 token | 中 | 汇总统计 |

**透明度设计原则**：

> **默认静默，按需详细**——日常工作不打扰，需要审计时能追溯。

##### 5.2 可控性（我能干预吗？）

| 控制点 | 场景 | 机制建议 |
|:-------|:-----|:---------|
| 开关 Summarizer | 怀疑摘要质量时 | 全局开关 |
| 清除特定缓存 | 文档更新后摘要过时 | 按文件/按问题清除 |
| 强制重新生成 | 测试新版 Summarizer | 忽略缓存选项 |
| 审阅摘要质量 | QA 摘要准确性 | 查看缓存内容界面 |

##### 5.3 成本感知（这要花多少钱/时间？）

**成本构成**：
- Token 消耗（Summarizer 本身是 LLM 调用）
- 时间成本（延迟）
- 维护成本（缓存管理）

**成本感知设计**：

```
[会话结束时汇总]
本次会话：
- Summarizer 调用: 5 次
- 缓存命中: 3 次 (60%)
- 新生成摘要: 2 次
- 估计节省 token: ~8000 (vs 直接 read_file)
- Summarizer 消耗 token: ~2000
- 净节省: ~6000 token
```

> **心理账户效应**：人类需要看到"节省"而非只看到"消耗"，才会觉得值得。

#### 六、综合建议

**体验设计原则**：

1. **Pit of Success**：让"正确使用 Summarizer"成为最自然的选择
2. **Graceful Degradation**：Summarizer 不可用时，自动回退到 read_file
3. **Progressive Disclosure**：默认简洁，详情按需展开
4. **Trust Through Transparency**：通过可审计性建立信任

**MVP 体验规格建议**：

| 功能 | MVP 必须 | MVP 可选 | 未来 |
|:-----|:---------|:---------|:-----|
| 调用标记 | ✅ 显示"via Summarizer" | | |
| 缓存状态 | ✅ 命中/未命中标记 | | |
| 进度反馈 | ✅ 生成中提示 | | |
| 成本汇总 | | ✅ 会话结束统计 | |
| 缓存管理 | | ✅ 清除命令 | |
| 质量评分 | | | 用户反馈机制 |

**我的判断**：

从体验角度，Summarizer 最大的风险不是技术实现，而是 **信任建立**。

> 如果 Agent 发现 Summarizer 丢了关键信息，即使只发生一次，也会形成"不可靠"的心智模型，之后就不愿意用了。

因此我建议：

1. **宁可少摘要，不可错摘要**——保守优于激进
2. **明确标注不确定性**——"这部分我不确定是否完整"
3. **提供逃生通道**——"如需完整内容，请直接阅读原文"
4. **建立反馈闭环**——让 Agent 能报告"摘要遗漏了 X"

---
