# 会议：App-For-LLM 进程架构决策

> **时间**: 2025-12-12
> **主持**: Team Leader (刘德智)
> **参会**: Planner, Investigator, GeminiAdvisor, CodexReviewer
> **议题**: App-For-LLM 应统一设计为独立进程，还是分为内嵌与外部两种？

---

## 背景

**App-For-LLM** 是 Agent 系统的扩展机制，封装"数据+视图+操作"为一个整体供 LLM 使用。与 LLM 交互的界面叫 DocUI（渲染+操作）。

### 当前技术栈
- **PipeMux**: CLI ↔ Broker ↔ Backend Apps（独立进程模式）
- **已有原型**: MemoryNotebook, TextEditor, SystemMonitor（都是独立进程）

### 核心问题
| 选项 | 描述 |
|------|------|
| **A. 统一独立进程** | 所有 App 都是独立进程，通过 IPC（Named Pipe/JSON-RPC）与 Agent 通信 |
| **B. 混合模式** | 内嵌 App（Agent 进程内）+ 外部 App（独立进程）两种形式并存 |

### 关键考量
1. **内嵌的独特价值**: 自省/反射类功能（如 Recap 维护滚动历史摘要）需要直接操作 Agent 内部状态（History）
2. **开发者生态**: 鼓励第三方贡献，LLM Agent 群体自身可能是主力开发者
3. **性能**: LLM 平均 30 秒调用一次工具，通讯开销可忽略
4. **资源**: 进程开销不是主要考量；资源受限环境为次要考虑

### 请各位 Specialist 从自己的专业视角分析

---

## 第一轮发言

### Planner

**倾向: 方案 A（统一独立进程）**

**决策维度识别**:
| 维度 | 权重 | 说明 |
|------|------|------|
| 语义自然度 | 高 | 内嵌是否是某些场景的唯一自然表达 |
| API 统一性 | 中 | 两套机制是否增加开发者认知负担 |
| 能力边界 | 中 | 外部进程能否覆盖内嵌的所有场景 |
| 生态摩擦 | 高 | 第三方开发者（含 LLM Agent）的贡献阻力 |
| 演进灵活性 | 中 | 架构决策对未来扩展的约束 |

**关键洞察**: Recap 需要"直接操作 Agent 内部状态"可以拆解为：
- **读取 History**: 可通过 Agent 暴露只读 API 实现
- **写入/修改 History**: 这是真正的"特权操作"，可通过 RPC 调用实现

**方案 C（变体）**: 外部进程 + 特权 API
- 统一使用外部进程
- Agent 为特殊需求暴露特权 API 通道
- 保持进程模型统一，特权行为显式声明

**理由**:
1. 奥卡姆剃刀：在证明需要混合模式之前，不引入额外复杂度
2. PipeMux 投资复用：现有架构和 SDK 已足够成熟
3. 生态友好：一套规则对第三方开发者最友好

**建议**: 先用方案 A 实现 Recap 原型，验证"操作 History"的摩擦点，再做最终决策。

---

### Investigator

**倾向: 方案 B（混合模式）**

**业界调研结果**:

| 系统 | 选择 | 原因 | 教训 |
|------|------|------|------|
| **VS Code** | 混合（偏独立进程） | 隔离性、安全性、性能 | 需精细设计 API 边界 |
| **Emacs/Vim** | 纯内嵌 | 历史、深度集成、即时性 | 稳定性依赖插件质量 |
| **LSP** | 强制独立进程 | 语言无关、复用、隔离 | 通信延迟、状态同步复杂 |
| **浏览器扩展** | 强制独立进程 + 沙箱 | 安全至上、多租户 | 能力受限 |
| **LangChain/AutoGPT** | 纯内嵌 | 开发简单、LLM 开发者友好 | 恶意 Tool、依赖冲突 |

**启示**: 
- 当性能不是瓶颈、社区以"快速原型"为主时，内嵌模式的低门槛更有价值
- 参考 VS Code：内嵌 App 用于自省/反射类功能，外部 App 用于调用外部服务
- 关键设计：内嵌 App 的 API 设计要谨慎，参考 VS Code 的 `vscode.d.ts` 边界划分

---

### GeminiAdvisor

**倾向: 方案 A+（统一独立进程 + 高权限 API）**

**核心论点**: 不应仅因"奥卡姆剃刀"选择 A，而应因**"LLM 作为开发者"**这一关键约束。

**挑战 Investigator**: 
> VS Code 的插件主要是人类写的，经过了测试。而我们的目标是让 LLM 自己写 App。LLM 写出的代码大概率包含死循环、内存泄漏或异常。
> 
> 在混合模式下，一个内嵌的、由 LLM 刚刚生成的"Recap App"如果崩溃，会导致整个 Agent 进程崩溃（"一尸两命"）。

**独特视角**:
1. **沙箱即防线**: 独立进程天然提供资源隔离，Agent 可以直接 `kill` 掉卡死的 App
2. **语言无关性**: 方案 A 允许 App 使用任何语言（Python 数据科学 App、Rust 性能关键 App）
3. **热重载与试错**: Agent 可以毫秒级启动新进程测试 App，失败了就丢弃；内嵌模块动态加载/卸载极其复杂
4. **协议即文档**: LLM 更容易理解显式的 JSON-RPC 协议，难以理解内存钩子

**UX 视角**: 方案 A 迫使 App 开发者设计清晰的数据接口（View Model），避免 dump 内部状态污染 Context Window。

**具体方案**:
- 统一协议：所有 App 通过 Stdio/Socket 与 Agent 通信
- Agent 即服务：暴露高权限 API（如 `GET /history`, `GET /memory-tree`），Recap 通过 API 拉取状态

**一句话**: 为了让 LLM 能够安全地为自己编写扩展，我们需要**进程级的隔离**；为了满足深度功能，我们需要**API 级的开放**，而不是**内存级的耦合**。

---

### CodexReviewer

**倾向: 方案 A**

**实现复杂度对比**:
- **方案 A**: 核心工作在 IPC 协议与生命周期管理，无进程内 ABI 兼容负担，调试与崩溃隔离简单
- **方案 B**: 需要两套运行时（进程内 + 进程外），同一 API 需双实现；内嵌 App 要处理线程安全、内存泄漏、全局状态污染。工程成本至少是 A 的 2-3 倍

**技术债务**:
- **A**: 债务主要在协议演化与权限模型，因为隔离清晰，债务可控
- **B**: 债务集中在进程内嵌入的隐性耦合，随着内嵌 App 增多，回归成本显著上升

**高权限 API 设计建议**:
1. **能力授予模型**: capability token + 租约（lease），显式申请与时效性
2. **通信层**: 单一 IPC 渠道，高权限调用需追溯日志与速率限制
3. **历史访问**: 默认只读；写入需幂等接口，附带变更理由和追溯 ID
4. **版本演化**: API 分层（Core/HighPriv/Experimental），接口带 schema 版本

**边界案例**:
- **A 失败场景**: 需要极低延迟回调（<10ms）或与宿主 UI 紧耦合；受限环境禁止 spawn 进程
- **B 失败场景**: 内嵌 App 崩溃拖垮 Agent；内嵌升级导致 ABI 冲突；安全核查要求强隔离

**改进建议**:
- 先落地 A：定义最小可用协议、高权限能力申请流程与追溯格式
- 建立 SDK：生成器 + 类型安全封装，提供本地回放/模拟器
- 安全前置：默认最小权限，高权限调用需上下文、rate limit、追溯落盘

---

## 第一轮小结

| Specialist | 倾向 | 核心理由 |
|------------|------|----------|
| **Planner** | A | 奥卡姆剃刀，验证再复杂化 |
| **Investigator** | B | 业界调研显示内嵌有独特价值 |
| **GeminiAdvisor** | A+ | LLM 作为开发者需要进程级隔离容错 |
| **CodexReviewer** | A | 工程实现更可控，技术债更易管理 |

**共识点**:
1. 性能不是决策因素（30 秒间隔）
2. 需要某种方式支持 Recap 等自省功能
3. 安全和隔离很重要（尤其 LLM 生成代码场景）

**分歧点**:
- Investigator 认为参考 VS Code 的混合模式更灵活
- 其他三位认为"LLM 作为开发者"这一约束决定了应优先隔离

---

## 第二轮讨论

### Investigator 回应

> **立场修正：从 B 转向 A+，附加逃生舱条款**

**对"LLM 作为开发者"的回应**：

> 我的调研基于人类开发者为主的生态（VS Code、Emacs），但 GeminiAdvisor 指出的场景完全不同：**App 代码由 LLM 即时生成、未经测试、随时可能崩溃**。
> 
> 这不是"插件质量参差不齐"，而是"代码生成即执行"。在这个约束下，内嵌模式的低门槛优势被"一尸两命"的风险完全抵消。

**内嵌不可替代的条件？**

CodexReviewer 已识别：<10ms 延迟回调 或 宿主 UI 紧耦合。但这两个条件在我们的场景下几乎不成立：
- LLM 调用间隔 30 秒，IPC 延迟可忽略
- DocUI 是纯文本 Markdown，无"紧耦合 UI"概念

**建议的逃生舱条款**：

> 保留内嵌 App 作为 Agent 核心团队的内部实验通道，**不对外暴露 API**。仅用于验证"是否真的存在必须内嵌的场景"。如果 6 个月内无实际用例，移除这一通道。

---

### Planner 评估逃生舱条款

**评价：合理且审慎，但需微调执行细节**

**合理性 ✓**：这是典型的**低成本期权策略**——以极低成本保留未来选择权。在不确定"是否存在必须内嵌的场景"时，这是理性选择。

**风险点**：
1. 6 个月可能太长——建议 3 个月初审 + 6 个月终审
2. "无实际用例"定义需明确——是"团队尝试后发现不需要"还是"压根没人试"？
3. 隐性成本——即使不对外暴露，代码维护仍有成本

**决策论视角**：
- 符合 **regret minimization** 原则：6 个月后无论结果如何，都不会因"过早封死"或"投入过多"而后悔

**改进建议**：添加"实验必须主动进行"——指定 1-2 个具体场景尝试，而非被动等待用例浮现。

---

## 会议总结

### 最终共识：方案 A+（统一独立进程 + 特权 API）+ 逃生舱条款

| 组件 | 决策 |
|------|------|
| **进程模型** | 所有公开 App 均为独立进程 |
| **通信协议** | JSON-RPC over Named Pipe/Stdio（复用 PipeMux） |
| **自省功能** | 通过 Agent 暴露的特权 API 实现，而非内嵌 |
| **逃生舱** | 内部保留内嵌实验通道，3 个月初审，6 个月终审 |

### 决策依据

1. **"LLM 作为开发者"是核心约束**（GeminiAdvisor 提出，全员认可）
   - LLM 生成的代码需要进程级隔离容错
   - 内嵌模式的"一尸两命"风险不可接受

2. **工程成本差异显著**（CodexReviewer 分析）
   - 方案 B 的双轨实现成本是 A 的 2-3 倍
   - 技术债务更难管理

3. **性能非瓶颈**（全员共识）
   - 30 秒调用间隔下，IPC 开销可忽略

4. **业界经验支持隔离**（Investigator 调研）
   - LSP、浏览器扩展都选择了独立进程
   - VS Code 虽混合，但主力是独立进程

### 后续行动

| 项目 | 负责 | 截止 |
|------|------|------|
| 定义最小可用协议（启动/心跳/日志） | Implementer | TBD |
| 设计特权 API（History 只读/写入） | Planner → Implementer | TBD |
| 用方案 A 实现 Recap 原型验证 | Implementer | TBD |
| 指定 1-2 个内嵌实验场景 | Team Leader | 下次 Sprint Planning |

### 关键洞察存档

> **"进程级的隔离 + API 级的开放，而不是内存级的耦合"** — GeminiAdvisor
> 
> 这一表述精准概括了方案 A+ 的设计哲学：
> - **隔离**：保护 Agent 主进程不受 App 崩溃影响
> - **开放**：通过 API 满足深度定制需求
> - **解耦**：App 不依赖 Agent 内部实现细节

---

*会议结束于 2025-12-12*
*记录人：Team Leader (刘德智)*
